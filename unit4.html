<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unit 4: Language Models | NLP Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="container navbar">
            <a href="index.html" class="logo">NLP Portfolio</a>
            <nav>
                <ul class="nav-links">
                    <li><a href="index.html">Home</a></li>
                    <li><a href="about.html">About the Course</a></li>
                    <li><a href="objectives.html">Learning Objectives</a></li>
                    <li class="dropdown">
                        <a href="#">Units</a>
                        <div class="dropdown-content">
                            <a href="unit1.html">Unit 1</a>
                            <a href="unit2.html">Unit 2</a>
                            <a href="unit3.html">Unit 3</a>
                            <a href="unit4.html">Unit 4</a>
                            <a href="unit5.html">Unit 5</a>
                        </div>
                    </li>
                    <li><a href="reflection.html">Reflection</a></li>
                    <li><a href="achievements.html">Achievements</a></li>
                </ul>
            </nav>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </header>

    <section class="hero">
        <div class="container">
            <h1>Unit 4: Language Models</h1>
            <p>9 Hours</p>
        </div>
    </section>

    <main class="container">
        <section class="section">
            <div class="unit-content">
                <h2>Unit Overview</h2>
                <p>This unit focuses on advanced neural network approaches for language modeling. Students learn about recurrent neural networks, attention mechanisms, transformer architectures, and state-of-the-art language models that have revolutionized NLP in recent years.</p>
                
                <div class="download-section">
                    <a href="Short notes/UNIT_4_NLP.pdf" class="btn" target="_blank">Download Unit 4 Notes (PDF)</a>
                </div>
                
                <h3>Recurrent Neural Networks (RNN)</h3>
                <p>Recurrent Neural Networks are designed to process sequential data by maintaining a hidden state that captures information from previous inputs:</p>
                <ul>
                    <li>Basic RNN architecture and formulation</li>
                    <li>Backpropagation through time (BPTT)</li>
                    <li>Vanishing and exploding gradient problems</li>
                    <li>Applications in language modeling and sequence generation</li>
                </ul>
                
                <h3>Long Short-Term Memory (LSTM)</h3>
                <p>LSTMs are a specialized type of RNN designed to address the vanishing gradient problem:</p>
                <ul>
                    <li>LSTM architecture and components (input gate, forget gate, output gate, cell state)</li>
                    <li>Forward and backward pass in LSTMs</li>
                    <li>Variants of LSTM (peephole connections, coupled input-forget gates)</li>
                    <li>Bidirectional LSTMs</li>
                    <li>Applications in NLP tasks</li>
                </ul>
                
                <h3>Attention Mechanism</h3>
                <p>Attention mechanisms allow models to focus on different parts of the input sequence when generating output:</p>
                <ul>
                    <li>Motivation and intuition behind attention</li>
                    <li>Types of attention (additive, multiplicative, scaled dot-product)</li>
                    <li>Attention weights and context vectors</li>
                    <li>Applications in machine translation and other sequence-to-sequence tasks</li>
                </ul>
                
                <h3>Transformer Based Models</h3>
                <p>Transformers are a neural network architecture that relies entirely on attention mechanisms:</p>
                <ul>
                    <li>Transformer architecture overview</li>
                    <li>Encoder and decoder components</li>
                    <li>Positional encoding</li>
                    <li>Multi-head attention</li>
                    <li>Feed-forward networks</li>
                    <li>Layer normalization and residual connections</li>
                </ul>
                
                <h3>Self-attention</h3>
                <p>Self-attention allows models to relate different positions in a sequence to compute a representation:</p>
                <ul>
                    <li>Self-attention mechanism formulation</li>
                    <li>Query, key, and value matrices</li>
                    <li>Scaled dot-product attention</li>
                    <li>Masking in self-attention</li>
                </ul>
                
                <h3>Multi-headed Attention</h3>
                <p>Multi-headed attention allows models to attend to information from different representation subspaces:</p>
                <ul>
                    <li>Parallel attention heads</li>
                    <li>Concatenation and projection of attention outputs</li>
                    <li>Benefits of multiple attention heads</li>
                    <li>Implementation details</li>
                </ul>
                
                <h3>BERT</h3>
                <p>Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based language model:</p>
                <ul>
                    <li>BERT architecture and pre-training objectives (masked language modeling, next sentence prediction)</li>
                    <li>WordPiece tokenization</li>
                    <li>Special tokens ([CLS], [SEP], [MASK])</li>
                    <li>BERT variants (BERT-base, BERT-large)</li>
                    <li>Pre-training and fine-tuning methodology</li>
                </ul>
                
                <h3>RoBERTa</h3>
                <p>Robustly Optimized BERT Pretraining Approach (RoBERTa) is an optimized version of BERT:</p>
                <ul>
                    <li>Improvements over BERT (training methodology, data size, dynamic masking)</li>
                    <li>Removal of next sentence prediction objective</li>
                    <li>Performance comparison with BERT</li>
                    <li>Implementation considerations</li>
                </ul>
                
                <h3>Fine Tuning for Downstream Tasks</h3>
                <p>This section covers how pre-trained language models can be adapted for specific NLP tasks:</p>
                <ul>
                    <li>Transfer learning in NLP</li>
                    <li>Fine-tuning methodology and best practices</li>
                    <li>Task-specific layers and outputs</li>
                    <li>Hyperparameter selection for fine-tuning</li>
                    <li>Evaluation of fine-tuned models</li>
                </ul>
                
                <h3>Text Classification</h3>
                <p>Text classification involves assigning predefined categories to text documents:</p>
                <ul>
                    <li>Classification tasks (sentiment analysis, topic classification, intent detection)</li>
                    <li>Feature extraction for text classification</li>
                    <li>Traditional vs. neural approaches</li>
                    <li>Fine-tuning transformer models for classification</li>
                    <li>Evaluation metrics for text classification</li>
                </ul>
                
                <h3>Text Generation</h3>
                <p>Text generation involves creating coherent and contextually appropriate text:</p>
                <ul>
                    <li>Autoregressive language modeling</li>
                    <li>Decoding strategies (greedy, beam search, sampling)</li>
                    <li>Conditional text generation</li>
                    <li>Evaluation of generated text</li>
                    <li>Applications of text generation</li>
                </ul>
                
                <div class="navigation-links">
                    <a href="unit3.html" class="btn">Previous: Unit 3</a>
                    <a href="objectives.html" class="btn">Back to Learning Objectives</a>
                    <a href="unit5.html" class="btn">Next: Unit 5</a>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; Saiyyam Ahuja | NLP Course Portfolio</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>